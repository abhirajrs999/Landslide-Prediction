# -*- coding: utf-8 -*-
"""ICEM_Team2_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e99uuAFAFIE3I2yTasinHXGozCDkNCpM

# **Landslide Prediction using ML Models**

## Import libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.ensemble import AdaBoostClassifier

from sklearn import datasets
from mlxtend.plotting import plot_decision_regions
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier


from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

import warnings

warnings.filterwarnings('ignore')

"""## Import dataset"""

df= pd.read_csv('/content/dataset.csv')
df

# view dimensions of dataset

df.shape

"""There are 10 variables in the dataset. 8 are feature vectors. FOS(Factor of Safety) was calculated using the feature values. The discrete variable is `Labels` variable. It is also the target variable.The labels used are 0 (negative) and 1 (positive)."""

col_names = df.columns
col_names

# check distribution of Labels column
df['Labels'].value_counts()

# view the percentage distribution of Labels column
df['Labels'].value_counts()/np.float(len(df))

"""The percentage of observations of the class label `0` and `1` is 63.97% and 36.03%. So, the class is imbalanced."""

df.info()

"""There are no missing values in the dataset as the Non-Null count is equal to the total no of examples."""

round(df.describe(),2)

"""### Outliers in numerical variables"""

# draw boxplots to visualize outliers

plt.figure(figsize=(24,20))


plt.subplot(4, 2, 1)
fig = df.boxplot(column='Gradient')
fig.set_title('')
fig.set_ylabel('Gradient')


plt.subplot(4, 2, 2)
fig = df.boxplot(column='Soil Density')
fig.set_title('')
fig.set_ylabel('Soil Density')


plt.subplot(4, 2, 3)
fig = df.boxplot(column='Angle of friction')
fig.set_title('')
fig.set_ylabel('Angle of friction')


plt.subplot(4, 2, 4)
fig = df.boxplot(column='Rainfall')
fig.set_title('')
fig.set_ylabel('Rainfall')


plt.subplot(4, 2, 5)
fig = df.boxplot(column='Cohesion')
fig.set_title('')
fig.set_ylabel('Cohesion')


plt.subplot(4, 2, 6)
fig = df.boxplot(column='Soil water density')
fig.set_title('')
fig.set_ylabel('Soil water density')


plt.subplot(4, 2, 7)
fig = df.boxplot(column='Soil depth')
fig.set_title('')
fig.set_ylabel('Soil depth')


plt.subplot(4, 2, 8)
fig = df.boxplot(column='Water table height')
fig.set_title('')
fig.set_ylabel('Water table height')

"""The above boxplots suggest that there are not much outliers in the data.

### Check the distribution of variables
"""

# plot histogram to check distribution


plt.figure(figsize=(24,20))


plt.subplot(4, 2, 1)
fig = df['Gradient'].hist(bins=20)
fig.set_xlabel('Gradient')



plt.subplot(4, 2, 2)
fig = df['Soil Density'].hist(bins=20)
fig.set_xlabel('Soil Density')



plt.subplot(4, 2, 3)
fig = df['Angle of friction'].hist(bins=20)
fig.set_xlabel('Angle of friction')




plt.subplot(4, 2, 4)
fig = df['Rainfall'].hist(bins=20)
fig.set_xlabel('Rainfall')




plt.subplot(4, 2, 5)
fig = df['Cohesion'].hist(bins=20)
fig.set_xlabel('Cohesion')




plt.subplot(4, 2, 6)
fig = df['Soil water density'].hist(bins=20)
fig.set_xlabel('Soil water density')




plt.subplot(4, 2, 7)
fig = df['Soil depth'].hist(bins=20)
fig.set_xlabel('Soil depth')



plt.subplot(4, 2, 8)
fig = df['Water table height'].hist(bins=20)
fig.set_xlabel('Water table height')

"""All the 8 features are skewed.

## 9. Declare feature vector and target variable
"""

X = df.drop(columns = ['FOS','Labels'], axis=1)
y = df['Labels']

"""## 10. Split data into separate training and test set"""

# split X and y into training and testing sets
#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 80)
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state=42)

X_train.shape, X_test.shape

"""## 11. Feature Scaling"""

cols = X_train.columns
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

X_train = scaler.fit_transform(X_train)

X_test = scaler.transform(X_test)

X_train = pd.DataFrame(X_train, columns=[cols])

X_test = pd.DataFrame(X_test, columns=[cols])

"""# LOGISTIC REGRESSION"""

model = LogisticRegression()
model.fit(X_train, y_train)
X_train_prediction = model.predict(X_train)
training_data_accuracy = accuracy_score(X_train_prediction, y_train)
print('Accuracy on training data : ', training_data_accuracy)
X_test_prediction = model.predict(X_test)
pred_log=X_test_prediction
test_data_accuracy = accuracy_score(X_test_prediction, y_test)
acc_log=test_data_accuracy
print('Accuracy on test data : ', test_data_accuracy)

"""# RANDOM FOREST"""

forest = RandomForestClassifier(criterion='gini', n_estimators=4, random_state=42, n_jobs=2)
forest.fit(X_train, y_train)
X_train_prediction = forest.predict(X_train)
training_data_accuracy = accuracy_score(X_train_prediction, y_train)
print('Accuracy on training data : ', training_data_accuracy)
X_test_prediction = forest.predict(X_test)
pred_rand=X_test_prediction
test_data_accuracy = accuracy_score(y_test,X_test_prediction)
acc_rand=test_data_accuracy
print('Accuracy on test data : ', test_data_accuracy)
#y_pred = forest.predict(X_test)
#print('Accuracy: %.3f' % accuracy_score(y_test, y_pred))

"""# ADABOOST"""

def get_models():
	models = dict()
	# define number of trees to consider
	n_trees = [10, 50, 100, 500, 1000]
	for n in n_trees:
		models[str(n)] = AdaBoostClassifier(n_estimators=n)
	return models

# evaluate a given model using cross-validation
def evaluate_model(model, X, y):
	# define the evaluation procedure
	cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)
	# evaluate the model and collect the results
	scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv)
	return scores

# get the models to evaluate
models = get_models()
# evaluate the models and store results
results, names = list(), list()
for name, model in models.items():
	# evaluate the model
	scores = evaluate_model(model, X, y)
	# store the results
	results.append(scores)
	names.append(name)
	# summarize the performance along the way
	print('>%s %.4f' % (name, np.mean(scores)))

#Since accuracy of 1000 trees is highest, predicting labels for the model with 1000 trees
model = AdaBoostClassifier(n_estimators=1000)
# fit the model on the whole dataset
model.fit(X_train, y_train)
# Prediction
y_pred = model.predict(X_test)
pred_adab=y_pred
acc_adab=0.9359



"""## Run SVM with rbf kernel and C=1.0, 10.0, 100.0, 1000.0 and gamma=`auto` among other parameters."""

# import SVC classifier
from sklearn.svm import SVC
# import metrics to compute accuracy
from sklearn.metrics import accuracy_score


#C=1.0
svc=SVC()
svc.fit(X_train,y_train)
y_pred=svc.predict(X_test)
print('Model accuracy score with default hyperparameters: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))

#C=10.0
svc=SVC(C=10.0)
svc.fit(X_train,y_train)
y_pred=svc.predict(X_test)
print('Model accuracy score with rbf kernel and C=10.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))

#C=100.0
svc=SVC(C=100.0)
svc.fit(X_train,y_train)
y_pred=svc.predict(X_test)
print('Model accuracy score with rbf kernel and C=100.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))

#C=1000
svc=SVC(C=1000.0)
svc.fit(X_train,y_train)
y_pred=svc.predict(X_test)
print('Model accuracy score with rbf kernel and C=1000.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))

"""## 13. Run SVM with linear kernel and C=1.0, 10.0, 100.0, 1000.0

"""

#C=1.0
linear_svc=SVC(kernel='linear', C=1.0)
linear_svc.fit(X_train,y_train)
y_pred_test=linear_svc.predict(X_test)
pred_svm=y_pred_test
acc_svm=(accuracy_score(y_test, y_pred_test))
print(acc_svm)
print('Model accuracy score with linear kernel and C=1.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred_test)))


#C=10.0
linear_svc10=SVC(kernel='linear', C=10.0)
linear_svc10.fit(X_train, y_train)
y_pred=linear_svc10.predict(X_test)
print('Model accuracy score with linear kernel and C=10.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))


#c=100.0
linear_svc100=SVC(kernel='linear', C=100.0)
linear_svc100.fit(X_train, y_train)
y_pred=linear_svc100.predict(X_test)
print('Model accuracy score with linear kernel and C=100.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))


#C=1000.0
linear_svc1000=SVC(kernel='linear', C=1000.0)
linear_svc1000.fit(X_train, y_train)
y_pred=linear_svc1000.predict(X_test)
print('Model accuracy score with linear kernel and C=1000.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))

"""### Comparing the train-set and test-set accuracy"""

y_pred_train = linear_svc.predict(X_train)

y_pred_train

print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))

"""The training set and test-set accuracy are very much comparable.

### Check for overfitting and underfitting
"""

# print the scores on training and test set

print('Training set score: {:.4f}'.format(linear_svc.score(X_train, y_train)))

print('Test set score: {:.4f}'.format(linear_svc.score(X_test, y_test)))

"""### Running SVM with polynomial kernel and C=1.0 and 100.0"""

#C=1.0
poly_svc=SVC(kernel='poly', C=1.0)
poly_svc.fit(X_train,y_train)
y_pred=poly_svc.predict(X_test)
print('Model accuracy score with polynomial kernel and C=1.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))


#C=100.0
poly_svc=SVC(kernel='poly', C=100.0)
poly_svc.fit(X_train,y_train)
y_pred=poly_svc.predict(X_test)
print('Model accuracy score with polynomial kernel and C=100.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))

"""## Confusion matrix


A confusion matrix is a tool for summarizing the performance of a classification algorithm.
Four types of outcomes are possible while evaluating a classification model performance. These four outcomes are:-

**True Positives (TP)**

**True Negatives (TN)**

**False Positives (FP)**

**False Negatives (FN)**
"""

# Print the Confusion Matrix and slice it into four pieces

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, y_pred_test)

print('Confusion matrix\n\n', cm)

print('\nTrue Positives(TP) = ', cm[0,0])

print('\nTrue Negatives(TN) = ', cm[1,1])

print('\nFalse Positives(FP) = ', cm[0,1])

print('\nFalse Negatives(FN) = ', cm[1,0])

"""The confusion matrix shows `3289 + 230 = 3519 correct predictions` and `17 + 44 = 61 incorrect predictions`.


In this case, we have


- `True Positives` (Actual Positive:1 and Predict Positive:1) - 3289


- `True Negatives` (Actual Negative:0 and Predict Negative:0) - 230


- `False Positives` (Actual Negative:0 but Predict Positive:1) - 17 `(Type I error)`


- `False Negatives` (Actual Positive:1 but Predict Negative:0) - 44 `(Type II error)`
"""

# visualize confusion matrix with seaborn heatmap

cm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive:1', 'Actual Negative:0'],
                                 index=['Predict Positive:1', 'Predict Negative:0'])

sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')

"""### Classification Report


**Classification report** is another way to evaluate the classification model performance. It displays the  **precision**, **recall**, **f1** and **support** scores for the model. I have described these terms in later.

We can print a classification report as follows:-
"""

from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred_test))

"""### Classification accuracy"""

TP = cm[0,0]
TN = cm[1,1]
FP = cm[0,1]
FN = cm[1,0]

# print classification accuracy

classification_accuracy = (TP + TN) / float(TP + TN + FP + FN)

print('Classification accuracy : {0:0.4f}'.format(classification_accuracy))

"""### Classification error"""

# print classification error

classification_error = (FP + FN) / float(TP + TN + FP + FN)

print('Classification error : {0:0.4f}'.format(classification_error))

"""### Precision"""

# print precision score

precision = TP / float(TP + FP)


print('Precision : {0:0.4f}'.format(precision))

"""### Recall"""

recall = TP / float(TP + FN)

print('Recall or Sensitivity : {0:0.4f}'.format(recall))

"""### True Positive Rate"""

true_positive_rate = TP / float(TP + FN)


print('True Positive Rate : {0:0.4f}'.format(true_positive_rate))

"""### False Positive Rate"""

false_positive_rate = FP / float(FP + TN)


print('False Positive Rate : {0:0.4f}'.format(false_positive_rate))

"""### Specificity"""

specificity = TN / (TN + FP)

print('Specificity : {0:0.4f}'.format(specificity))

"""### ROC Curve


Another tool to measure the classification model performance visually is **ROC Curve**.
The **ROC Curve** plots the **True Positive Rate (TPR)** against the **False Positive Rate (FPR)** at various threshold levels.

"""

from sklearn.metrics import roc_curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_test)
plt.figure(figsize=(6,4))
plt.plot(fpr, tpr, linewidth=2)
plt.plot([0,1], [0,1], 'k--' )
plt.rcParams['font.size'] = 12
plt.xlabel('False Positive Rate (1 - Specificity)')
plt.ylabel('True Positive Rate (Sensitivity)')
plt.show()

from sklearn.metrics import roc_auc_score
ROC_AUC = roc_auc_score(y_test, y_pred_test)
print('ROC AUC : {:.4f}'.format(ROC_AUC))

"""The higher the value of ROC AUC, the better the classifier. ROC AUC of our model approaches towards 1. So, we can conclude that our classifier does a good job.

k-fold cross-validation is a very useful technique to evaluate model performance. But, it fails here because we have a imbalanced dataset.

### Stratified k-Fold Cross Validation with shuffle split with  linear kernel
"""

from sklearn.model_selection import KFold


kfold=KFold(n_splits=5, shuffle=True, random_state=0)


linear_svc=SVC(kernel='linear')


linear_scores = cross_val_score(linear_svc, X, y, cv=kfold)

# print cross-validation scores with linear kernel

print('Stratified cross-validation scores with linear kernel:\n\n{}'.format(linear_scores))

# print average cross-validation score with linear kernel

print('Average stratified cross-validation score with linear kernel:{:.4f}'.format(linear_scores.mean()))

"""### Stratified k-Fold Cross Validation with shuffle split with rbf kernel"""

rbf_svc=SVC(kernel='rbf')


rbf_scores = cross_val_score(rbf_svc, X, y, cv=kfold)

# print cross-validation scores with rbf kernel

print('Stratified Cross-validation scores with rbf kernel:\n\n{}'.format(rbf_scores))

# print average cross-validation score with rbf kernel

print('Average stratified cross-validation score with rbf kernel:{:.4f}'.format(rbf_scores.mean()))

"""Higher average stratified k-fold cross-validation score of 0.8846 with linear kernel but the model accuracy is 0.9899.
So, stratified cross-validation technique does not help to improve the model performance.
"""

print(pred_log)
print(pred_rand)
print(pred_svm)
print(pred_adab)
sum_final=pred_log*acc_log+pred_rand*acc_rand+pred_svm*acc_svm+pred_adab*acc_adab
#sum_final=pred_log+pred_rand+pred_svm+pred_adab
#print(sum_final)

sum_final=sum_final/4

final_pred=np.zeros(99)
c=0
for x in sum_final:
  if(x>=0.5):
    final_pred[c]=1
    c=c+1
  else:
    final_pred[c]=0
    c=c+1

print(final_pred)

test_data_accuracy = accuracy_score(final_pred, y_test)
print(test_data_accuracy)